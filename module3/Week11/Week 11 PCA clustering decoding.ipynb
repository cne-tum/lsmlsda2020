{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import flammkuchen as fl\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = 1.5\n",
    "dt_imaging = 1/fps\n",
    "data_root = Path(r\"C:\\Users\\vilim\\analysis\\lsmlsda_data\\whole_brain\")\n",
    "traces = fl.load(str(data_root / \"traces_better_deconvolved.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_traces = zscore(traces, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear out traces with NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = np.logical_not(np.any(np.isnan(normalized_traces), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_traces = normalized_traces[sel, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rois, n_t_imaging = traces.shape\n",
    "t_imaging = np.arange(n_t_imaging)*dt_imaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = fl.load(str(data_root / \"coords.h5\"))[sel, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data (so that each trace has a mean 0 and variance 1) and plot all traces together as a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "heatmap = ax.imshow(normalized_traces, aspect=\"auto\", vmin=-2, vmax=2, cmap=\"RdBu_r\", extent=[0, t_imaging[-1], 0, n_rois])\n",
    "ax.set_xlabel(\"Time [s]\")\n",
    "ax.set_ylabel(\"ROI #\")\n",
    "fig.colorbar(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this part we will correlate the individual traces (original traces, not the ones averaged over trials) with sensory and motor regressors.\n",
    "# To do so, fist load the behavioural log and stimulus log\n",
    "stimulus_log = fl.load(data_root / \"stimulus_log.h5\")\n",
    "behavior_log = fl.load(data_root / \"behavior_log.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_double_decimate(t_orig, sig_orig, t_imaging, n_decimate=5):\n",
    "    dt_imaging = t_imaging[1]-t_imaging[0]\n",
    "    t_imaging_int = np.arange(len(t_imaging)*n_decimate**2)*dt_imaging/(n_decimate**2) # we decimate twice as the procedure works only for decimation amount > 13\n",
    "    # (see https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.decimate.html)\n",
    "\n",
    "    interpolated = interp1d(t_orig, sig_orig, bounds_error=False, fill_value=0)(t_imaging_int)\n",
    "\n",
    "    return signal.decimate(signal.decimate(interpolated, n_decimate, ftype=\"iir\"), n_decimate, ftype=\"iir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the regressors\n",
    "### Motor regressor\n",
    "The motor regressor we sould like to have will be a general measure of the fish swimming power. Such regressor can be based on the standart deviation (SD) of the tail angle during the experiment. \n",
    "The behaviour of the fish was recorded and saved in the file \"behavioural_log\". In this DataFrame you will see the diffeent angles of the segments of the fish tail, as well as the variable \"tail_sum\". The motor regressor should be a moving SD of tail_sum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the motor regressor \n",
    "\n",
    "tail_sum = behavior_log['tail_sum'].values\n",
    "\n",
    "dt_beh = np.mean(np.diff(behavior_log.t[100:200]))\n",
    "vig_win = 0.05\n",
    "n_vig = int(vig_win/dt_beh)\n",
    "vigor = behavior_log.tail_sum.rolling(n_vig,  min_periods=1).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(behavior_log.t, vigor)\n",
    "plt.xlabel(\"time [s]\")\n",
    "plt.ylabel(\"vigor [a.u.]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensory regressors\n",
    "Creating two regressors for the stimulus (stimulus speed).\n",
    "From the stimulu_log, get the variable \"gain_kag_cl1D_vel\". This is the velocity of the moving gratings. We will use this trace to create two regressors - one for positive velocity and one for negative velocity. Use the interpolation me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we resample the stimulation data so that it is equaly spaced in time, at 200 times the imaging frame rate (another method is the one demonstrated above for vigor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity = upsample_double_decimate(stimulus_log.t, stimulus_log.gain_lag_cl1D_vel, t_imaging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the coordinate system convention, **negative velocity is forward** for the fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(t_imaging, velocity)\n",
    "plt.xlabel(\"t [s]\")\n",
    "plt.ylabel(\"v [mm/s]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_vel = zscore(np.maximum(velocity, 0))\n",
    "negative_vel = zscore(np.maximum(-velocity, 0))\n",
    "vigor = zscore(upsample_double_decimate(behavior_log.t.values, np.nan_to_num(vigor), t_imaging))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlating the traces with the regressors\n",
    "At this point you will correlate each calcium trace with the three regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = np.column_stack([positive_vel, negative_vel, vigor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_results = pd.DataFrame((normalized_traces @ regressors)/n_t_imaging, columns=[\"vel_pos\", \"vel_neg\", \"vigor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the best fitted neuron for each of the regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_indices = np.nanargmax(regression_results.values, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, sharex=True)\n",
    "for i_reg, (ax, name) in enumerate(zip(axes, regression_results.columns)):\n",
    "    ax.plot(t_imaging, normalized_traces[best_indices[i_reg], :], label=\"flourscence\")\n",
    "    ax.plot(t_imaging, regressors[:, i_reg], label=\"regressor\")\n",
    "    ax.set_ylabel(name)\n",
    "axes[0].legend()\n",
    "axes[2].set_xlabel(\"t [s]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show regression coefficients in brain coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_results = pd.concat([regression_results,\n",
    "                               pd.DataFrame(coords, columns=[\"z\", \"y\", \"x\"])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, sharex=True, sharey=True)\n",
    "for i_reg, (ax, name) in enumerate(zip(axes, regression_results.columns)):\n",
    "    ax.scatter(regression_results.x, regression_results.y, c=regression_results[name], s=0.2, vmin=-1, vmax=1, cmap=\"RdBu_r\")\n",
    "    ax.set_title(name)\n",
    "    ax.set_aspect(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward velocity and vigur are very correlated, which is why their maps look almost the same. Doing a proper multi-linear regression with sparsity constraints might allow us to tease apart the two cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create trial-averaged traces. Each trial is 180 seconds. This will show a cleaner stimulus-related response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 9\n",
    "trial_duration = 180.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_t_trial = n_t_imaging // n_trials\n",
    "\n",
    "grouped_traces = np.reshape(normalized_traces, (-1, n_trials, n_t_trial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_per_trial = np.mean(grouped_traces, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_imaging_trial = np.arange(traces_per_trial.shape[1])*dt_imaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "heatmap = ax.imshow(traces_per_trial, aspect=\"auto\", vmin=-2, vmax=2, cmap=\"RdBu_r\", extent=[0, t_imaging_trial[-1], 0, n_rois])\n",
    "ax.set_xlabel(\"Time [s]\")\n",
    "ax.set_ylabel(\"ROI #\")\n",
    "fig.colorbar(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction and clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract principal components of the average response.components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(100)\n",
    "loadings = pca.fit_transform(traces_per_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the first 3 PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(pca.components_[:3, :].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the variance explained by each component and try to establish how many components you need to explain everything that is not noise. Extra credit: do cross validated PCA (fit the PCs on average traces of some trials, and check how many components you need to explain other trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you interpret the principal components in terms of stimulus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PC trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the neural activity of the whole brain as a phase-space plot (extra credit: encode time or stimulus value in the color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(pca.components_[0, :], pca.components_[1, :], color=\"gray\", lw=0.5)\n",
    "plt.scatter(pca.components_[0, :], pca.components_[1, :], c=velocity[:len(t_imaging_trial)])\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use K means clustering to classify neurons by principal component loading (using all components that are not noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the neurons in the space of principal component loading coefficients (for PC1 and PC2) and color them by cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_ids = KMeans(3).fit_predict(loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(loadings[:,0], loadings[:, 2], c=clust_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the clusters showing discrete response classes? What are the assumptions of K-Means and does this datasat satisfy it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters in anatomical space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(in the readme now there is a link to the coords file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = 0.6\n",
    "dy = 0.6\n",
    "dz = 7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2)\n",
    "ax[0,0].scatter(coords[:,1]*dx, coords[:,2]*dy, s=0.1, c=clust_ids)\n",
    "ax[0,0].set_aspect(1)\n",
    "ax[0,0].set_xticklabels('')\n",
    "\n",
    "ax[1,0].scatter(coords[:,1]*dx, coords[:,0]*dz, s=0.1, c=clust_ids)\n",
    "ax[1,0].set_aspect(1)\n",
    "\n",
    "ax[0,1].scatter(coords[:,0]*dz, coords[:,2]*dy, s=0.1, c=clust_ids)\n",
    "ax[0,1].set_aspect(1)\n",
    "ax[0,1].set_yticklabels('')\n",
    "ax[1,1].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, color the cells according to principal component loading or cluster assignement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode the velcity from the traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the velocity and the traces into a traning and test set. Choose carefully so that most conditions are well represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "all_trials = np.random.permutation(n_trials)\n",
    "n_test = 2\n",
    "\n",
    "trials_train = all_trials[n_test:]\n",
    "trials_test = all_trials[:n_test]\n",
    "\n",
    "traces_train, traces_test = (np.concatenate([grouped_traces[:, i_trial, :]\n",
    "                                             for i_trial in sel_trials], axis=1)\n",
    "                             for sel_trials in [trials_train, trials_test])\n",
    "\n",
    "\n",
    "vel_train, vel_test = (np.concatenate([velocity[i_trial*n_t_trial:(i_trial+1)*n_t_trial]\n",
    "                                             for i_trial in sel_trials], axis=0)\n",
    "                             for sel_trials in [trials_train, trials_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use methods from scikit-learn, starting with sklearn.linear_model.LinearRegression (or write your own linear regression!), use the fit and predict methods to decode velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.g. for a linear model:\n",
    "    $$v(t) = \\Sigma_{i}^{n\\_neurons}w_i a_i(t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of features (>8000!), so we need to prevent overfitting by penalizing large weights. Scikit-learn provides a cross-validated version of sum-of-square weights penalized linear regression, `RidgeCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = RidgeCV(alphas=[1.0, 10.0, 100.0,1000.0, 10000.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.fit(traces_train.T, vel_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vel_test_pred = decoder.predict(traces_test.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the decoded velocity vs the real velocity, in time and as a scatter plot. Which regions of the stimulus space are decoded best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(vel_test_pred, label=\"prediction\")\n",
    "plt.plot(vel_test, label=\"true velocity\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra credit \n",
    "* try to determine how many cells you need to decode the velocity. Which cells are the most important ones, if there are such?\n",
    "* do nonlinear decoding methods (e.g. neural networks, also available with the same interface in scikit-learn) improve the decoding?\n",
    "* try to decode behavior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

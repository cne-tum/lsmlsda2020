{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter inference in models of decision making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to week 3 of module 1! \n",
    "\n",
    "As discussed in the lecture, a central challenge in mechanistic modeling is to identify parameters that are in agreement with model and data using Bayesian inference. This is especially challenging in mechanistic models, since the likelihood function is commonly intractable. \n",
    "\n",
    "After working with a complex mechanistic model for the last two weeks, this week we will turn to simpler mechanistic model already discussed in the lecture - the drift diffusion model. We chose this simple mechanistic model because it is very fast in generating data (especially compared to the Wong & Wang model from last week). This will enable you to try out different methods for Bayesian parameter inference yourself: A classical approach called rejection ABC and a more modern one, which is based on density estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple mechanistic model of decision making: The Drift diffusion model (DDM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the DDM as a simple example:\n",
    "\n",
    "The DDM simulates a perceptual decision making process in a [two alternative forced choice experiment (2AFC)](https://en.wikipedia.org/wiki/Two-alternative_forced_choice) with a single scalar variable $x$. For example, the the task for the subject could be to report the direction of movement of a cloud of points in which a certain amount of points move left or right. \n",
    "\n",
    "You can think of $x$ as the sensory evidence towards one or the other choice: it starts at a neutral position, e.g., at zero, and moves up or down following a mean drift $\\mu$ (the first parameter of the model) plus some noise (external and internal noise) $\\sigma$ (the second parameter of the model) and a decision towards one or the other side is made when a pre-defined decision boundary is hit.\n",
    "\n",
    "In a way it is performing a random walk with a drift towards one or the other choice ([more details](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process)), that is why it is called drift ($\\mu$) and diffusion ($\\sigma$) model. There are more reasons for this name, the interested reader is referred to [here](https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation).\n",
    "\n",
    "Formally, the drift diffusion model is defined through the following equation: \n",
    "$$ \n",
    "\\begin{align}\n",
    "\\text{d}X = \\mu \\text{dt} + \\sigma \\text{dW}\n",
    "\\end{align}\n",
    "$$\n",
    "where $X$ is the decision variable, $\\mu$ the drift, and $\\sigma$ the diffusion parameter (effectively scaling the variance of the noise coming from the [Wiener process](https://en.wikipedia.org/wiki/Wiener_process) W). This equation is a stochastic differential equation (SDE) with no closed-form solution. Therefore, we must use a SDE solver to generate data: given a drift $\\mu$ and a diffusion parameter $\\sigma$ we integrate the equation for a given number of time steps and obtain the trace of $X$. Then the reaction time is given by the time when $X$ crossed the predefined decision boundary and the decision is given by the sign of the $X$ at that time. \n",
    "\n",
    "- You can find a short introduction to the DDM model in the Neural Dynamics book (16.4.2): https://neuronaldynamics.epfl.ch/online/Ch16.S4.html Note that they use a slightly different notation. \n",
    "- There are three additional packages you need to install in your conda environment for this exercise: \n",
    "  - To simulate the DDM, please install `sdeint`, e.g., with `pip install sdeint`\n",
    "  - For neural network training, install pytorch, following [this guide](https://pytorch.org/get-started/) and selecting your platform, `Conda`, and `None` for the `CUDA` option. We also recommend getting familiar with the syntax of PyTorch, which is very close to NumPy, e.g., by following this tutorial: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "  - [tqdm](https://github.com/tqdm/tqdm) for progress bars: `conda install tqdm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Understand the model.\n",
    "\n",
    "To complete the exercises in this notebook we provide you with several function needed to generate data with the DDM. It is important that you understand what is going on in all of these functions.\n",
    "\n",
    "**Overall goal**: Understand the code, how the model generates data and how summary statistics are calculated. \n",
    "\n",
    "**Hint**: The functions all depend on each other. It can be helpful to execute each function with the inputs generated from other function in order to understand what they are doing. Have a look at exercises 1 b) and c) for an example.\n",
    "\n",
    "a) Read the code carefully and write [docstrings](https://www.datacamp.com/community/tutorials/docstrings-python)  and additional explanatory inline comments for all the functions. You can use any format for the docstring you like, [here is a style suggestion](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html). The docstrings should document what goes into each function and what is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to install the required packages so that all imports are available.\n",
    "import numpy as np\n",
    "import sdeint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "torch.set_default_tensor_type(torch.FloatTensor)  # Calculate on CPU rather than GPU per default\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for the DDM\n",
    "\n",
    "To run inference with the model, we need two functions, prior (the prior over parameters) and simulator (the model): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(num_samples=1):\n",
    "    dist = torch.distributions.Uniform(low=-2.0, high=2.0).expand([1])\n",
    "    return dist.sample((num_samples,))\n",
    "\n",
    "\n",
    "def simulator(parameters, num_trials=100, sigma=0.2, T=5, y0=0, a=1.0, return_traces=False, nbins=20, dt=0.01):\n",
    "    \n",
    "    if type(parameters) == float:\n",
    "        parameters = torch.tensor([parameters])\n",
    "    if parameters.ndim == 1:\n",
    "        parameters = parameters.reshape(1, -1)\n",
    "        \n",
    "    # number of parameters to simulate\n",
    "    num_parameters = parameters.shape[0]\n",
    "    \n",
    "    x_traces = []\n",
    "    stats = []\n",
    "\n",
    "    # run simulation one by one as this is faster than batching in sdeint.\n",
    "    for idx_param in range(num_parameters): \n",
    "        \n",
    "        num_simulations = num_trials\n",
    "        mu = parameters[idx_param].numpy()\n",
    "\n",
    "        # this repeats each element in mu num_trials times.\n",
    "        mu_expanded = np.repeat(mu, num_trials)\n",
    "\n",
    "        tspan = np.linspace(0.0, T, int(T/dt))\n",
    "        y0_expanded = y0 * np.ones(num_simulations)\n",
    "\n",
    "        def f(x, t):\n",
    "            return np.eye(num_simulations).dot(mu_expanded)\n",
    "\n",
    "        def G(x,t):\n",
    "            return sigma * np.eye(num_simulations)\n",
    "    \n",
    "        x_traces_param = sdeint.itoEuler(f, G, y0_expanded, tspan).T\n",
    "\n",
    "        rts, decisions = find_rts_and_decisions(x_traces_param, 1, num_trials, tspan, a=a)\n",
    "        stats_param = calculate_histogram_stats(rts, decisions, T=T, nbins=nbins)\n",
    "        \n",
    "        # collect result as torch tensors (to be able to use PyTorch Neural Nets later on)\n",
    "        x_traces.append(torch.as_tensor(x_traces_param, dtype=torch.float32))\n",
    "        stats.append(torch.as_tensor(stats_param, dtype=torch.float32))\n",
    "    \n",
    "    x_traces = torch.cat(x_traces)\n",
    "    stats =  torch.cat(stats)\n",
    "    \n",
    "    if return_traces: \n",
    "        return x_traces, stats\n",
    "    else:\n",
    "        return stats\n",
    "\n",
    "\n",
    "def find_rts_and_decisions(x_traces, num_parameters, num_trials, tspan, a=1.0): \n",
    "    \n",
    "    assert x_traces.shape[0] == num_parameters * num_trials\n",
    "    \n",
    "    rts = []\n",
    "    decisions = []\n",
    "    \n",
    "    for param_idx in range(num_parameters):\n",
    "        \n",
    "        # get traces for this param.\n",
    "        x_traces_param = x_traces[param_idx * num_trials: (param_idx + 1)*num_trials, ]\n",
    "            \n",
    "        # find crossing of decision threshold\n",
    "        rows, _ = ((abs(x_traces_param) >= a)).nonzero()\n",
    "        # the unique rows are the ones with decisions. \n",
    "        unique_rows = np.unique(rows)\n",
    "        \n",
    "        # find undecisive trials\n",
    "        undecided_idx = [i for i in range(num_trials) if i not in unique_rows]\n",
    "        # enforce decision in last time bin. \n",
    "        x_traces_param[undecided_idx, -1] = a * np.sign(x_traces_param[undecided_idx, -1])\n",
    "        \n",
    "        # now search again for decision threshold, all trials are decisive now.\n",
    "        rows, cols = ((abs(x_traces_param) >= a)).nonzero()\n",
    "        # get first indices for every trial\n",
    "        unique_rows, trial_idx = np.unique(rows, return_index=True)\n",
    "        \n",
    "        # get first decision time bin idx for every trial\n",
    "        decision_idx = cols[trial_idx]\n",
    "        \n",
    "        # find proportion of up decisions by looking at sign at decision idx.\n",
    "        decisions_param = np.sign(x_traces_param[unique_rows, decision_idx])\n",
    "        rt_param = tspan[decision_idx]\n",
    "\n",
    "        # mark the direction of the decision by the multiplying the rt with the sign.\n",
    "        rts.append(rt_param)\n",
    "        decisions.append(decisions_param)\n",
    "        \n",
    "    return np.array(rts), np.array(decisions)\n",
    "\n",
    "\n",
    "def calculate_histogram_stats(rts, decisions, T, nbins=20): \n",
    "    # mark rts with decision direction sign \n",
    "    sign_rts = rts * decisions\n",
    "\n",
    "    # fixed bins\n",
    "    bins = np.linspace(-T, T, nbins)\n",
    "\n",
    "    counts = []\n",
    "    for trial_idx in range(sign_rts.shape[0]): \n",
    "        # count for every trial\n",
    "        trial_count, *_ = np.histogram(sign_rts[trial_idx, :], bins=bins)\n",
    "        counts.append(trial_count)\n",
    "    \n",
    "    return np.array(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 continued:\n",
    "\n",
    "Look at the two cells below. What is happening here? \n",
    "\n",
    "b) Describe what is happening in the first cell below.\n",
    "\n",
    "c) Describe the figure in the second cell below. Change the title and labels of the figure accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials, T, dt = 50, 5, 0.01\n",
    "x_traces, _ = simulator(torch.tensor([[-0.3], [0.3]]), num_trials=num_trials, T=T, dt=dt, return_traces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[18, 5])\n",
    "tspan = np.linspace(0, T, int(T/dt))\n",
    "\n",
    "plt.plot(tspan, x_traces[:num_trials, :].numpy().T, c=\"C0\")\n",
    "plt.plot(tspan, x_traces[num_trials:, :].numpy().T, c=\"C1\")\n",
    "plt.title(\"Title\", fontsize=20)\n",
    "plt.ylabel('y label', fontsize=20)\n",
    "plt.xlabel('x label [unit]', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) We defined two functions, `prior` and `simulator`. The prior returns draws from a uniform prior in [-2.0, +2.0] over parameters. Look at the three lines of code in the cell below. What is happening here? What does the output of the simulator represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = prior(1)\n",
    "stats = simulator(param)\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Parameter inference with rejection ABC\n",
    "\n",
    "In the lecture and the tutorial you learned about rejection ABC, the root of all simulation-based inference algorithms (more details in the lecture slides or [here](https://en.wikipedia.org/wiki/Approximate_Bayesian_computation#The_ABC_rejection_algorithm). Here, you will implement this algorithm and use it do inference over the drift parameter $\\mu$ of the DDM, given observed data $s_o$. \n",
    "\n",
    "**Overall goal**: Obtain 100 samples from the approximate posterior using rejection ABC.\n",
    "\n",
    "a) Write a function called `rejection_abc` with the function signature as in the cell below. The function takes as arguments a prior, simulator, observed_data, a simulation budget, a distance function and a quantile. The quantile is used to select the parameters according to the top quantile of the sorted distances. We provide the arguments and the return of the function, and the distance function. Make sure to write docstrings and comments in your function. (**Hint**: you do not need loops to implement this algorithm, you can pass all parameters at once to the simulator, or to the distance function.)\n",
    "\n",
    "b) Test your function with small simulation budgets to save time. Later, for the final inference, we suggest a budget of around 10000 simulations (this can take a couple of minutes to run). \n",
    "\n",
    "c) Run inference using the observed data and obtain 100 samples from the posterior (quantile=0.01 for a budget of 10000 samples). \n",
    "\n",
    "d) Make a pretty figure with title, axis labels and good fontsizes etc showing the histogram of accepted posterior samples. Plot the ground truth parameter $\\mu_o$ as vertical line. Are the posterior samples close to the ground truth? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The observed data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_o = torch.tensor([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 13., 22., 20., 14.,\n",
    "         11.,  7.,  4.,  4.,  4.]])\n",
    "mu_o = torch.tensor([[0.4588]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_distance(observation, simulated_data):\n",
    "    return torch.norm((observation - simulated_data), dim=-1)\n",
    "\n",
    "def rejection_abc(prior, simulator, observed_data, num_simulations, distance_fun, q):\n",
    "    \n",
    "    # Your code for rejection ABC.\n",
    "    # Hint: Note that prior, simulator and distance_fun all are callable functions, \n",
    "    # while observed_data is a tensor, num_simulations is a integer and q a float.\n",
    "    # Also note that most functions like \"sort, \"argsort\" you might know from numpy are \n",
    "    # available in PyTorch as well.\n",
    "    \n",
    "    return samples  # return accepted parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) run inference using your rejection abc function. \n",
    "\n",
    "# samples = rejection_abc(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# d) plot the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Parameter inference with Conditional Density Estimation\n",
    "\n",
    "Rejection ABC, as the name says, is based on rejecting samples, which can be very inefficient if the data is high dimensional. Alternative methods using neural networks for density estimation have been developed in recent years, starting with: https://papers.nips.cc/paper/6084-fast-free-inference-of-simulation-models-with-bayesian-conditional-density-estimation\n",
    "\n",
    "Here, we coded up a simple version of this approach: A neural network that takes the simulated data as input and regresses this input on the mean and std (in log-space) of a Normal distribution. The Normal distribution is then used to approximate the posterior distribution. \n",
    "\n",
    "After training, any observed data point can be passed to the neural network (now a conditional density estimator) and the it will return the parameters for the Normal distribution - for the corresponding posterior. Thus, by giving $s_o$ to the network one can obtain the posterior $p(\\mu | s_o)$. \n",
    "\n",
    "\n",
    "**Your exercises**: \n",
    "\n",
    "a) Read the code carefully to understand what is happening. Consider revisiting a [tutorial on Pytorch](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html).\n",
    "\n",
    "b) Use the `tqdm` package to update the code such that it shows progress bars of the training epochs.\n",
    "\n",
    "c) Train the network with 10000 simulations. Then get the posterior $p(\\mu | s_o)$ by passing the observed data `s_o` from above into the network and use the resulting parameters to instantiate a Normal distribution. Plot the probabilities across the support of the prior (-2. to 2.).\n",
    "\n",
    "d) Draw samples from the posterior and plot them in a histogram. Compare the histogram to the one you obtained by Rejection ABC. Do they differ? If so, what might be reasons for the difference? \n",
    "\n",
    "As always, make sure to add title and labels to your figures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is some PyTorch code training a conditional density estimator for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorDataset which we will use for training the density estimator\n",
    "# From a TensorDataset, we construct a DataLoader that allows splitting data into shuffled batches\n",
    "# during training of the NN. For background see PyTorch tutorials on data handling\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "# You might wanna change the num_samples argument for playing around. \n",
    "parameters = prior(num_samples=10000)\n",
    "data = simulator(parameters)\n",
    "\n",
    "train_data = TensorDataset(parameters, data)\n",
    "data_loader = DataLoader(train_data, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build a conditional density estimation neural network, which regresses on the mean and std (in log-space) of a Normal distribution: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = [50, 50]\n",
    "input_size = data.shape[1] \n",
    "\n",
    "# The NN takes data (observation) as input\n",
    "# There are 2 hidden layers of 50 units each\n",
    "# The regression is onto 2 numbers that will represent the parameters of a Normal distribution\n",
    "network = nn.Sequential(\n",
    "    nn.Linear(input_size, num_hiddens[0]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(num_hiddens[0], num_hiddens[1]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(num_hiddens[1], 2),\n",
    "    )\n",
    "\n",
    "# Create an optimizer\n",
    "optim = torch.optim.Adam(network.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to change this cell for exercise 3b. \n",
    "# import tqdm for progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Train the parameters of the NN in minibatches\n",
    "losses = []\n",
    "for epoch in range(100):\n",
    "    for inputs, outputs in data_loader:\n",
    "        nn_out = network(outputs)\n",
    "        cond_dist = torch.distributions.Normal(loc=nn_out[:,0], scale=torch.exp(nn_out[:,1]))\n",
    "        loss = -1. * cond_dist.log_prob(inputs.reshape(-1)).mean()\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "# Plot loss of NN training\n",
    "plt.figure(18, 5)\n",
    "plt.title('loss')\n",
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for exercise 3c:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for exercise 3d:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 (Optional) \n",
    "\n",
    "If you are interested in trying out more advanced CDE methods, we invite you to have a look at a toolbox we are currently developing in the mackelab, a toolbox for simulation-based inference, called `SBI`: https://github.com/mackelab/sbi \n",
    "\n",
    "This exercise is optional. You can have a look at the documentation page at https://www.mackelab.org/sbi/. If you want to proceed, you can clone the repository, follow the installation instructions and try to use the simulator and prior above to run inference on the DDM using `SBI`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
